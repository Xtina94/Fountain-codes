\subsubsection{Relay techniques}
collaboration
have been proposed to increase system performance
[2]. In this work, diversity is introduced at the packet level.
Instead of directly forwarding the received packet, the nodes
store and re-transmit a linear combination of the previously
received packets from their buffer. As a consequence, at the
destination, the packets are likely to contain diverse combinations
of the initial binary data: this is referred to as packet
diversity in this paper. By this way, redundancy is introduced
without the overload drawback of basic packet repetition.
In [3], the authors presented this technique for Random
Linear code (RL code). However, in a WSN with limited
resources, the use of RL code is unsuitable due to its high
decoding complexity. To avoid this problem, in this paper,
we propose to focus on a more practical code called Luby
Transform code (LT code). The idea proposed in [3], where relay nodes simply combine
packets using XOR operations, is more attractive for WSN.
Although the LT code itself has been shown to provide
several benefits for WSN [5], relaying schemes using an
arbitrary XOR operation between LT encoded packets are
subject to several constraints. Indeed, simply applying XOR
operation as in [3] becomes inefficient because the global
degree distribution of the information flow arriving at the
destination is altered and leads to decoding inefficiency.\\
\note{the Results:}\\
For any link within a linear network, we have considered an
erasure channel model characterized by the same Packet Error Rate (PER). To highlight the positive impact of our proposed
relaying strategies, we focus on a high PER. An extra header
of size K is added to each packet. The indices corresponding
to the fragments of the original packet that are contained in
the encoded packet are set to 1 while others are set to 0.
Consequently, the header of a XOR-ed packet results from a
XOR combination among the headers of the original packets.
We represent our results for two distinct cases when ML
and BP decoding are used as shown in Fig. 2 and Fig.
3 respectively. Although LT code is meant to be decoded
using BP algorithm, the results for the ML-decoding case are
presented as well. Indeed, contrary to BP, ML-decoding is not
biased by the shift in the degree distribution and thus provides
an optimal bound with respect to the coding overhead.
The hop-by-hop scenario gives the lowest bound on the
number of transmissions required from the source to achieve
transmission success. However, it comes at a price of too high
latency and computational cost. Hence, we aim at approaching
this optimum using alternative strategies with respect to WSNs
requirements. Our results are evaluated in term of the overload
factor which is defined as the ratio between the number
of transmissions required from the source for each strategy
and the number of transmissions required for the hop-by-hop
scenario. Our objective is to minimize this overload factor.
The results from Fig. 2 confirm that XOR-ing along the line
reduces overload factor thanks to packet diversity introduced
in the system. The more packets are combined together, the
higher the robustness becomes. This behavior is however
different with BP-decoding. For the XOR combination of 5 last
packets scenario, the overload factor is at its best with ML,
but for the BP-decoder, the severe alteration of the degree
distribution limits its ability to decode. The degradation of
degree distribution after 10 hops is shown in Fig. 4. Compared
to the RSD, strategies 3, 5 and 6 provide the mean square
error of the received degree distribution equal to 3.80 x 10-5 ,
8.14x10-4 and 6.25 x 10-5 respectively.
For the BP-decoder, the basic strategy where the Last Packet
is retransmitted performs better than the retransmission of a
XOR combination with prescribed degree. As we can see from
Fig. 4, low degree packets are difficult to be obtained from
Algorithm 1. In contrast, the LT-Adapted XOR combination
with prescribed degree method preserves the Robust Soliton
Distribution and leads to an efficient decoding process with a
high performance improvement. The optimal XOR-ing probability
(PXOR) for this specific configuration is approximately
0.2. With BP-decoding as shown in Fig. 3, apart from the hopby-
hop Decode and Forward strategy that is mentioned to be
unsuitable for WSN, our proposed relaying technique outperforms
all other realistic existing relaying schemes because it
represents a good tradeoff between an increased diversity and
the decoding capability.
\cri{An application: BER for unequal error protection}

Paper 14
\cri{Nontheless it is meaningful to go into details for what concerns Raptor codes}\\
Elias showed that the capacity of the BEC with erasure probability
equals . He further proved that random codes of
rates arbitrarily close to can be decoded on this channel
with an exponentially small error probability using maximumlikelihood
(ML) decoding. In the case of the erasure channel,
ML decoding of linear codes is equivalent to solving systems
of linear equations. This task can be done in polynomial time
using Gaussian elimination. However, Gaussian elimination is
not fast enough, especially when the length of the code is long.
Reed–Solomon codes can be used to partially compensate
for the inefficiency of random codes. Reed–Solomon codes can
be decoded from a block with the maximum possible number
of erasures in time quadratic in the dimension. (There are
faster algorithms based on fast polynomial arithmetic, but these
algorithms are often too complicated in practice.) However,
quadratic running times are still too large for many applications.
In [2], the authors construct codes with linear time encoding
and decoding algorithms that can come arbitrarily close to the
capacity of the BEC. These codes, called Tornado codes, are
very similar to Gallager’s low-density parity-check (LDPC)
codes [3], but they use a highly irregular weight distribution for
the underlying graphs.

schemes, a new class of codes is needed. Fountain codes
constitute such a class, and they address all the above mentioned
issues.

For many applications it is important to construct universal
Fountain codes for which the average weight of an output
symbol is a constant and which have fast decoding algorithms.
In this paper, we introduce such a class of Fountain codes,
called Raptor codes\\
The results of the previous section imply that LT-codes
cannot be encoded with constant cost if the number of collected
output symbols is close to the number of input symbols. tion 1. The decoding graph needs to have an order of
edges in order to make sure that all the input nodes are covered
with high probability. The idea of Raptor coding is to relax
this condition and require that only a constant fraction of the
input symbols be recoverable. Then the same information-theoretic
argument as before shows only a linear lower bound for
the number of edges in the decoding graph.
There are two potential problems with this approach: 1) The
information-theoretic lower bound may not be matchable with
an algorithm, and 2) we need to recover all the input symbols,
not only a constant fraction.
The second issue is addressed easily: we encode the input
symbols using a traditional erasure correcting code, and then
apply an appropriate LT-code to the new set of symbols in a
way that the traditional code is capable of recovering all the
input symbols even in face of a fixed fraction of erasures. To
deal with the first issue, we need to design the traditional code
and the LT-code appropriately.
Let be a linear code of block length and dimension , and
let be a degree distribution.ARaptor code with parameters
is an LT-code with distribution on symbols
which are the coordinates of codewords in . The code is
called the pre-code of the Raptor code. The input symbols of a
Raptor code are the symbols used to construct the codeword in
consisting of intermediate symbols.\\
D. Combined Error Probability
The decoding error probability of a Raptor code with parameters
can be estimated using the finite-length analysis
of the corresponding LT-code and of the pre-code . This
can be done for any code with a decoder for which the decoding
error probability is completely known. For example,
can be chosen from the ensemble .
We will assume throughout that the input symbols of the
Raptor code need to be recovered from output symbols.
Suppose that has block length . For any with , let
denote the probability that the LT-decoder fails after recovering
of the intermediate symbols. Further, let denote the
probability that the code cannot decode erasures at random
positions. Since the LT-decoding process is independent of the
choice of , the set of unrecovered intermediate symbols at the
point of failure of the decoder is random. Therefore, if denotes
the probability that the input symbols cannot be recovered
from the output symbols, then we have
Using the results of the previous two subsections, it is possible
to obtain good upper bounds on the overall error probability of
Raptor codes\\
obtain
the value of the output symbol.
Asimple probabilistic analysis shows that for maximum-likelihoos
(ML) decoding to have a vanishing error probability for
an LT-code, the average degree of an output symbol has to grow
at least logarithmically with the number of input symbols. This
makes it very difficult to obtain a linear time encoder and decoder
for an LT-code. Raptor codes [16] are an extension of
LT-codes which solve this problem and yield easy linear time
encoders and decoders. The main idea behind Raptor codes is to
pre-code the input symbols using a block code with a linear time
encoder and decoder. The output symbols are produced using
the original input symbols together with the redundant symbols
of the pre-code. Raptor codes solve the transmission problem
over an unknown erasure channel in an almost optimal manner,
as described in [16].
The success of Fountain codes for the erasure channel suggests
that similar results may also be possible for other binarysymmetric
channels. In this paper, we will investigate this question.
As we will show, some of the properties of LT- and Raptor
codes over the erasure channel can be carried over to any binary
input memoryless symmetric channels (BIMSC), while some
other properties cannot. \\
In this paper we will study BIMSCs. Three examples of such
channels are furnished by the BEC with erasure probability
, denoted BEC , the BSC with error probability , denoted
BSC , and the BIAWGN channel with variance , denoted
BIAWGN .
We consider transmission with binary antipodal signaling.
Strictly speaking, with this kind of signaling, we cannot speak
of XORing bits. However, we will abuse notation slightly and denote
the real product of the input values as the “XOR” of the bits.
The output of a BIMSC with binary input can be
identified with a pair , where , and is a
real number between and . The value is interpreted as
a guess of the input value before transmission over the channel,
and can be interpreted as the probability that the guess is incorrect.
The channel can be identified with the probability density
function (pdf) of the error probability . For example, BEC is
identified with the probability distribution ,\\

Finally, we give a formal definition of the reception overhead
of a decoder: For each received bit, let be the probability
that the bit was zero before transmission, and let
, where is the number of collected output
bits to which the decoding algorithm is to be applied. We say
that the decoding algorithm has a reception overhead of if
. In other words, the algorithm works with a
number of output bits that is only away from the optimal
number.
\note{The results}
In this section, we will report on simulations we performed
for degree distributions that were optimized for the BEC, as reported
in [16]. Our results are analogous to those of Palanki and
Yedidia [22].
Our experiments used the output distribution
(14)
We chose a Raptor code with parameters ,
where is a right-Poisson, left-regular LDPC code of rate
, as described in [16]. The communication channel is
BIAWGN , and the simulations were done for various values
of the standard deviation . The results of these simulations are
summarized in Fig. 2.\\
We performed our experiments in the following way: each
time we ran enough experiments to see 200 bit errors, or 2000
decoding runs, whichever was first, and we also ran at least 200
decoding runs. Then we calculated the average fraction of bit
errors at the end of the decoding process. During the decoding
process, we ran the BP algorithm for at most 300 rounds.
Since the degree distribution was optimized for the BEC, the
smallest overhead to ensure a good error probability is expected
to occur for the case ; this is also what the simulations
suggest. Moreover, as is increased, the corresponding overhead
needs to be increased as well.
The graphs in Fig. 2 clearly show the advantage of Raptor
codes over LT-codes.
