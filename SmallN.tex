\subsubsection{Optimal degree distribution for small message length codes}
\cri{Tieni a mente la questione del nome pacchetti. Perchè in questo caso non ci sono più i redundant packets. Allora, la nomencaltura è così: $k$ sono i source packets, che a dire il vero sono l'equivalente degli $N$ nel paper sulle Markov chain $d = 1,\dots,k$}
Another point of view is given in \cite{Hyytia2007} where, instead of focusing on the decoder, the authors decided to approach the problem from the perspective of finding the optimal degree distribution defining the number of blocks in each packet. They do so looking at the problem in two different ways: a Markov chain approach and a combinatorial one.

In the case of the Markov chain approach, from the receiver's point of view, the set of received and partially/fully decoded packets represents a state. At the same time, state transition probabilities depend on specific packets arrival probabilities, which in turn depend on the degree distribution used in the encoding process. From this it follows that the absorbing state is the one formed by the original blocks. It is worth notice that the state space needs only include the states that cannot be reduced by the decoder (\textit{irreducible}), plus further reductions can be performed playing on isomorphies on block paths as well as other tricks. However, the state space implosion is unavoidable and occurs already at the lowest values of number of blocks in the message, that is to say the number of input packets (defined $N$ from now on), whereby the analysis in done on a small number of them \cite{Hyytia2007}. After these steps the transition probability matrix can be constructed like:
\begin{equation}
  \textbf{P} =
  \begin{pmatrix}
    \boldsymbol{Q} & \boldsymbol{R}\\
    \boldsymbol{0} & \boldsymbol{I}\\
  \end{pmatrix}
\end{equation}
and we have $\boldsymbol{\pi_0} = (1,\dots,0)$ for the initial distribution vector and $\boldsymbol{\pi_{abs}} = (0,\dots,1)$ for the absorbing state. With the system set in this way, we can find the best degree distribution through two different criteria: either \textit{minimizing the avg number of steps} needed for decoding, $E[T]=\boldsymbol{\pi_0}(\boldsymbol{I}-\boldsymbol{Q})^{-1}\boldsymbol{e}^T$, or \textit{maximizing the success probability}, $\mathcal{P}_N = \boldsymbol{\pi_0 P}^N\boldsymbol{\pi}_{abs}$.

In the case of the combinatorial approach, the central observation is that in such case the order in which the packets are received is irrelevant. The recursive algorithm that derives from it aims at calculating the probability $\mathcal{P}_N$ that a message consisting of $N$ blocks is succesfully decoded with exactly $N$ received packets. So in general we take this probability $\mathcal{P}_N = P_N(p_1,\dots,p_N)$, with the trivial cases $P_0=1$ and $P_1(p_1) = p_1$ being the seeds for recursion, and condition it on $N-m$ of the $N$ received packets having degree 1. Moreover we need to specify that the fact of having no duplicates and being all the $n-m$ degree 1 packets distinct happens with probability $(N-1)!/m!n^{N-m-1}$. This is needed to weight the recursive part, which consists in the decoding problem for the other $m$ packets, whose degree can be lowered from being $\geq 2$ after removing the $N-m$ degree-1 packets. With these specifications, the recursive approach can be expressed as:
\begin{align}
    P_N(&p_1,\dots,p_n) = \sum_{m=0}^{N-1}\binom{N}{m}p_1^{N-m}(1-p_1)^m\\
    &\times\frac{(N-1)!}{m!N^{N-m-1}}P_m(p_1^{(N,m)},\dots,p_m^{(N,m)}),
    \label{eq:3}
\end{align}
where
\begin{equation}
  p_j^{(N,m)} = \sum_{i=2}^{N}\frac{p_i}{1-p_1}\frac{\binom{m}{j}\binom{N-m}{i-j}}{\binom{N}{i}}, \qquad j = 1,\dots,m.
  \label{eq:4}
\end{equation}
The first fraction in \autoref{eq:4} calculates the probability that a packet has degree $i$ conditioned to the fact that it is not of degree 1, while the second fraction is the probability that when "from an urn" containing $N$ blocks - $m$ of which are white - a degree-$i$ packet is constructed, then the subsequent reduced degree is exactly $j$\cite{Hyytia2007}.

\paragraph{The results}
The performance analysis have been done comparing the obtained optimal results to four main distributions: the uniform distribution, the degree-1 distribution ($p_i = 1 \quad (i=1)$), the binomial distribution and the Soliton distribution. In the case of the Markov approach the authors in \cite{Byers} show how the \textit{MinAvg} and the \textit{MaxPr} lead to similar results, which in general are better than the results from all the other distributions. In particular, \autoref{tab:distrib} shows how Binomial and Soliton distribution work still reasonably well, but not the same can be said regarding the other two distributions.
\begin{table}
\centering
\begin{tabular}{ccccccc}
\firsthline
& MinAvg & MAxPr & Binomial & Soliton & Uniform & Degree-1\\
\hline
$p_1$ & 0.524 & 0.517 & 3/7 & 1/3 & 1/3 & 1\\
$p_2$ & 0.366 & 0.397 & 3/7 & 1/2 & 1/3 & 0\\
$p_3$ & 0.109 & 0.086 &  1/7 & 1/6 & 1/3 & 0\\
\hline
$E[T]$ & 4.046 & 4.049 & 4.133 & 4.459 & 4.725 & 5.5\\
$\mathcal{P}_3$ & 0.451 & 0.452 & 0.437 & 0.397 & 0.354 & 0.222\\
\lasthline
\end{tabular}
  \caption{Optimal performance and weights in the case N = 3}
  \label{tab:distrib}
\end{table}
In the parallel case of the combinatorial approach, two main aspects are highlighted: first, with the state explosion not being such a problem now, the fact that we can analyze cases for values of $N$ up to $30$; second, being the eigenvalues of $A$ (The second derivative matrix of $\mathcal{P}_N$) a rapidly decreasing sequence, the function is extremely insensitive to changes along the last eigenvectors. This bringing to the result that typically three conditions are enough to define a close-to-the-optimum distribution \cite{Hyytia2007}.
