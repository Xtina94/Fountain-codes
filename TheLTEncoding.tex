\section{The LT-codes}
\label{sec:lt}
A further improvement in the field of digital fountain codes is given by the important calss of LT codes, ratless codes introduced and described by Luby in \cite{Luby}. LT codes are the first realization of universal erasure codes, meaning that the codes symbol length can be arbitrary. Moreover, these codes are rateless, which means that the number of encoding symbols that can be generated from the data is potentially limitless.

Comparing LT codes to the preceding Tornado codes, it can be seen that their analysis is slightly different: in particular the Tornado codes analysis can only be applied to graphs with constant maximum degree, while LT codes graphs are of logarithmic density. Moreover, while for the Tornado codes the reception overhead\footnote{The \textit{Reception overhead} of an algorithm is well described in \cite{Etesami2006}: having $p_i = $ the probability that a received bit was $0$ before transmission, $m =$ the number of collected output bits and $E = \sum_{i=1}^m(1-h(p_i))$, then the decoding algorithm has a reception overhead $\epsilon$ if $E = k(1+\epsilon)$, which is, the algorithm needs a number of output bits which is only $\epsilon$ away from the optimal number. The same definition can then be applied to the concept when speaking in term of packets, instead of bits.} is at least a constant fraction of the data length, for the LT codes it is an asymptotically fading fraction of the data length, yet with the tradeoff of slightly higher asymptotic encoding and decoding times. Another considerable advantage of these codes is that, once $k$ and $n$ have been chosen, the Tornado encoder generates $n$ symbols and cannot produce others on the fly like LT codes do. Finally, there is a notable difference in the degree distribution of input and encoding symbols: for Tornado codes the degree distribution on input symbols is similar to the so-called \textit{Soliton distribution}, while the degree distribution of encoding symbols can be approxhimated by the Poisson distribution. This thing, though, cannot be applied to the LT codes, since the Soliton distribution cannot be the resulting degree distribution on input symbols when encoding symbols are generated independently, regardless of the distribution of neighbors of encoding symbols.
\cite{Luby}

\subsection{The encoding process}
In \cite{Luby} Luby gives a complete explanation of the encoding and the decoding process. The former can be here briefly described by the following steps:
\begin{itemize}
  \item From a degree distribution, randomly choose the degree $d$ of the encoding symbol;
  \item Choose uniformly at random $d$ distinct input symbols that can be neighbors of the encoding symbol;
  \item Finally, perform the exclusive-or of the $d$ neighbors to obtain the value of the encoding symbol;
\end{itemize}
In this way from $N$ input symbols we can generate a theoretically infinite amount of encoding symbols, each one having a certain degree $d_i$ for $i=1,\dots,N$.

There are several degree distributions that can be used for our encoding symbols and the choice of each one comes with different performance in terms of coding times, efficiency and \gls{ber}, which we will see further on in this work. In general the most important ones are:
\begin{itemize}
  \item The \textit{All-At-Ones distribution}: $\rho(d) = 1$;
  \item The \textit{Ideal Soliton Distribution}: $\rho(1) = 1/k$ and $\forall i = 2,\dots,k$ then $\rho(i) = 1/i(i-1)$;
  \item The \textit{Robust Soliton Distribution}:
  \begin{itemize}
    \item $\beta  = \sum_{i=1}^{k}\rho(i)+\tau(i)$
    \item $\forall i = 1,\dots,k$ then $\mu(i) = \frac{(\rho(i)+\tau(i))}{\beta}$
  \end{itemize}
  with the quantity $\tau(i)$ being:\\
  \[
  \tau(i) =
  \begin{cases}
    R/ik \qquad \text{for } i = 1,\dots,k/R-1\\
    Rln(R/\delta)/k \qquad \text{for } i = k/R\\
    0 \qquad \text{for } i = k/R +1, \dots,k
  \end{cases}
  \]
  for $R = c\cdot ln(k/\delta)\sqrt k$ for some suitable constant $c > 0$
\end{itemize}
